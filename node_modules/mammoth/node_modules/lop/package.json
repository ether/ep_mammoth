{
  "name": "lop",
  "version": "0.2.8",
  "author": {
    "name": "Michael Williamson",
    "email": "mike@zwobble.org"
  },
  "description": "Create parsers using parser combinators with helpful error messages",
  "keywords": [
    "parse",
    "parser",
    "combinator"
  ],
  "repository": {
    "type": "git",
    "url": "https://github.com/mwilliamson/lop.git"
  },
  "dependencies": {
    "underscore": "~1.4.4",
    "option": "~0.2.1",
    "duck": "~0.1.11"
  },
  "devDependencies": {
    "nodeunit": "~0.8.0"
  },
  "license": "BSD",
  "readme": "# lop -- parsing library for JavaScript\n\nlop is a library to create parsers using parser combinators with helpful errors.\n\n```javascript\n\nfunction parse(tokens) {\n    var parser = lop.Parser();\n    return parser.parseTokens(expressionRule, tokens);\n}\n\n// This rule is wrapped inside lop.rule to defer evaluation until\n// the rule is used -- otherwise, it would reference integerRule\n// and ifRule, which don't exist yet.\nvar expressionRule = lop.rule(function() {\n    return rules.firstOf(\"expression\",\n        integerRule,\n        ifRule\n    );\n});\n\nvar integerRule = rules.then(\n    rules.tokenOfType(\"integer\"),\n    function(value) {\n        return new IntegerNode(parseInt(value, 10));\n    }\n);\n\nvar ifRule = rules.sequence(\n    rules.token(\"keyword\", \"if\"),\n    rules.sequence.cut(),\n    rules.sequence.capture(expressionRule),\n    rules.token(\"keyword\", \"then\"),\n    rules.sequence.capture(expressionRule),\n    rules.token(\"keyword\", \"else\"),\n    rules.sequence.capture(expressionRule)\n).map(function(condition, trueBranch, falseBranch) {\n    return new IfNode(condition, trueBranch, falseBranch);\n});\n```\n\nlop tries to provide helpful errors where possible. For instance, in `ifRule`\nas defined above, there is a cut following the keyword `if`. Before the cut,\nif we fail to match the input, we can backtrack -- in this case, we backtrack\nand see if another form of expression might match the input. However, after the\ncut, we prevent backtracking. Once we've see the keyword `if`, there's no doubt\nabout which sort of expression this is, so if parsing fails later in this rule,\nthere's no point in backtracking. This allows informative error messages to be\ngenerated: if we try to parse the string `\"if 1 42 else 12\"`, we get the error:\n\n    Error: File: /tmp/lop-example\n    Line number: 1\n    Character number: 6:\n    Expected keyword \"then\"\n    but got integer \"42\"\n\n## Tokens\n\nWhen using a parser built with lop, the input is an array of tokens. A token can be any value so long as it has the property `source`, which must be a `StringSourceRange`. For instance, to create a simple tokeniser that generates a stream of words tokens separated by whitespace tokens:\n\n```javascript\nvar StringSource = require(\"lop\").StringSource;\n\nfunction tokeniseString(string) {\n    return tokenise(new StringSource(string, \"raw string\"));\n}\n\nfunction tokenise(source) {\n    var string = source.asString();\n    var whitespaceRegex = /(\\s+)/g;\n    var result;\n    var start = 0;\n    var parts = [];\n    \n    while ((result = whitespaceRegex.exec(source)) !== null) {\n        parts.push({\n            type: \"word\",\n            value: string.substring(start, result.index),\n            source: source.range(start, result.index)\n        });\n        parts.push({\n            type: \"whitespace\",\n            value: result[1],\n            source: source.range(result.index, whitespaceRegex.lastIndex)\n        });\n        start = whitespaceRegex.lastIndex;\n    }\n    parts.push({\n        type: \"word\",\n        value: string.substring(start),\n        source: source.range(start, string.length)\n    });\n    parts.push({\n        type: \"end\",\n        source: source.range(string.length, string.length)\n    });\n    return parts.filter(function(part) {\n        return part.type !== \"word\" || part.value !== \"\";\n    });\n}\n```\n\nlop also defines its own notion of a token. Each instance of `lop.Token` has a type, name, and source, similarly to most of the tokens that would be created by the token above. For instance, instead of:\n\n    {\n        type: \"word\",\n        value: value,\n        source: source\n    }\n\nyou could use:\n\n    new Token(\"word\", value, source)\n\nThe main advantage of using `lop.Token` is that you can then use the rules `lop.rules.token` and `lop.rules.tokenOfType` (described later). If you don't use `lop.Token`, you must define your own atomic rules, but you can use the other rules without any modifications.\n\n## Parser\n\nTo parse an array of tokens, you can call the method `parseTokens` on `lop.Parser`, passing in the parsing rule and the array of tokens. For instance, assuming we already have a `tokenise` function (the one above would do fine):\n\n```javascript\nfunction parseSentence(source) {\n    var tokens = tokenise(source);\n    var parser = new lop.Parser();\n    var parseResult = parser.parseTokens(sentenceRule, tokens);\n    if (!parseResult.isSuccess()) {\n        throw new Error(\"Failed to parse: \" + describeFailure(parseResult));\n    }\n    return parseResult.value();\n}\n\nfunction describeFailure(parseResult) {\n    return parseResult.errors().map(describeError).join(\"\\n\");\n   \n    function describeError(error) {\n        return error.describe();\n    }\n}\n```\n\nThe result of parsing can be success, failure, or error. While failure indicates\nthat the rule didn't match the input tokens, error indicates that the input\nwas invalid in some way. In general, rules will backtrack when they\nencounter a failure, but will completely abort when they encounter an error.\nEach of these results has a number of methods:\n\n```javascript\n    result.isSuccess() // true for success, false otherwise\n    result.isFailure() // true for failure, false otherwise\n    result.isError() // true for error, false otherwise\n    result.value() // if success, the value that was parsed\n    result.remaining() // if success, the tokens that weren't consumed by parsing\n    result.source() // the StringSourceRange containing the consumed tokens\n    result.errors() // if failure or error, an array of descriptions of the failure/error\n```\n\nThe final question is then: how do we define rules for the parser, such as the currently undefined `sentenceRule`?\n\n## Rules\n\nEach rule in lop accepts an iterator over tokens, and returns a result, as\ndescribed in the previous section.\n\n### lop.rules.token(*tokenType*, *value*)\n\nSuccess if the next token has type `tokenType` and value `value`, failure\notherwise. Value on success is the value of the token.\n\n### lop.rules.tokenOfType(*tokenType*)\n\nSuccess if the next token has type `tokenType`, failure otherwise. Value on\nsuccess is the value of the token.\n\n### lop.rules.firstOf(*name*, *subRules*)\n\nTries each rule in `subRules` on the input tokens in turn. We return the result\nfrom the first sub-rule that returns success or error. In other words, return the\nresult from the first sub-rule that doesn't return failure. If all sub-rules return\nfailure, this rule returns failure.\n\n### lop.rules.then(*subRule*, *func*)\n\nTry `subRule` on the input tokens, and if successful, map over the result. For\ninstance:\n\n```javascript\nlop.rules.then(\n    lop.rules.tokenOfType(\"integer\"),\n    function(tokenValue) {\n        return parseInt(tokenValue, 10);\n    }\n)\n```\n\n### lop.rules.optional(*subRule*)\n\nTry `subRule` on the input tokens. If the sub-rule is successful with the value\n`value`, then return success with the value `options.some(value)`. If the sub-rule fails, return\nsuccess with the value `options.none`. If the sub-rules errors, return that error.\n",
  "readmeFilename": "README.md",
  "_id": "lop@0.2.8",
  "_from": "lop@~0.2.8"
}
